{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOi9r1gjducVWoQHZoCnyTQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n","!pip install pyaudio pydub\n","!pip install --quiet --upgrade diffusers transformers accelerate cohere openai openai-whisper"],"metadata":{"id":"ZSReiltW3lA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9WJ7cse3XhI"},"outputs":[],"source":["# realtime audio to image with a christmas theme using Stable Diffusion Turbo and Whisper\n","# by Ed Fries\n","# public domain\n","\n","import random, sys, torch\n","from diffusers import AutoPipelineForText2Image\n","from IPython.display import Javascript\n","from google.colab import output\n","from base64 import b64decode\n","from io import BytesIO\n","from pydub import AudioSegment\n","\n","RECORD = \"\"\"\n","const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n","const b2text = blob => new Promise(resolve => {\n","  const reader = new FileReader()\n","  reader.onloadend = e => resolve(e.srcElement.result)\n","  reader.readAsDataURL(blob)\n","})\n","var record = time => new Promise(async resolve => {\n","  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n","  recorder = new MediaRecorder(stream)\n","  chunks = []\n","  recorder.ondataavailable = e => chunks.push(e.data)\n","  recorder.start()\n","  await sleep(time)\n","  recorder.onstop = async ()=>{\n","    blob = new Blob(chunks)\n","    text = await b2text(blob)\n","    resolve(text)\n","  }\n","  recorder.stop()\n","})\n","\"\"\"\n","\n","def record(sec=3):\n","  #print(\"Speak Now...\")\n","  display(Javascript(RECORD))\n","  sec += 1\n","  s = output.eval_js('record(%d)' % (sec*1000))\n","  #print(\"Done Recording !\")\n","  b = b64decode(s.split(',')[1])\n","  return b #byte stream\n","\n","# Change these variables to customize your experience\n","bonusPrompt = \"a christmas themed \" # this string is prepended to the prompt sent to SD. Change to anything you want to give your pictures a consistent theme\n","X=512       # you can use a larger size but SD Turbo makes better images at 512x512 resolution\n","Y=512\n","whisperModel = \"openai/whisper-large-v3\"\n","whisperDevice = 'cuda' #'cpu' or 'cuda'\n","runLocal = False    # Set this to True to run without needing to be connected to the internet.\n","timeoutLength = 5   # This changes how long it collects audio information before passing it to Whisper. Try 5 for short phrases, 15 for longer phrases.\n","\n","big=False  #set big=True for 16gb graphics cards\n","if big:\n","    overscale=2 #adjust to fill your screen\n","    sdModel = \"stabilityai/sdxl-turbo\"\n","else:\n","    overscale=2\n","    sdModel = \"stabilityai/sdxl-turbo\"\n","\n","def InitRender():\n","    global pipe, font, scrn, info\n","    pipe = AutoPipelineForText2Image.from_pretrained(sdModel, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\", local_files_only = runLocal)\n","    pipe = pipe.to(\"cuda\")\n","\n","def RenderImage(prompt):\n","    seed = random.randint(0, sys.maxsize)\n","\n","    images = pipe(\n","        prompt = prompt,\n","        guidance_scale = 0.0,\n","        width = X,\n","        height= Y,\n","        num_inference_steps = 4,\n","        generator = torch.Generator(\"cuda\").manual_seed(seed),\n","        ).images\n","    images[0].save(\"output.jpg\")\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from PIL import Image\n","\n","def ShowImage(caption):\n","    img = Image.open('output.jpg')\n","    #img.thumbnail((1024,1024))\n","    plt.imshow(img)\n","    plt.axis('off')\n","    plt.show()\n","\n","from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n","import whisper\n","from tempfile import NamedTemporaryFile\n","import io\n","\n","def init_hear_text():\n","    global data_queue\n","    global source\n","    global temp_file\n","    global transcription\n","    global phrase_timeout\n","    global whisperPipe\n","    argsmodel = whisperModel\n","    argsnon_english = False\n","\n","    # Load / Download model\n","    model = AutoModelForSpeechSeq2Seq.from_pretrained(whisperModel, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n","    model.to(whisperDevice)\n","    processor = AutoProcessor.from_pretrained(whisperModel)\n","    whisperPipe = pipeline(\n","        \"automatic-speech-recognition\",\n","        model=model,\n","        tokenizer=processor.tokenizer,\n","        feature_extractor=processor.feature_extractor,\n","        max_new_tokens=128,\n","        chunk_length_s=30,\n","        batch_size=16,\n","        return_timestamps=True,\n","        torch_dtype=torch.float16,\n","        device=whisperDevice)\n","\n","    temp_file = NamedTemporaryFile().name\n","    transcription = ['']\n","\n","def hear_text():\n","    audio = record(3)\n","    with open(temp_file, 'w+b') as f:\n","      f.write(audio)\n","    result = whisperPipe(temp_file)\n","    text = result['text'].strip()\n","    return(text)\n","\n","# main program starts here\n","InitRender()\n","init_hear_text()\n","\n","while True:\n","    prompt = \"\"\n","    #print(\"listening...\")\n","    while prompt == \"\":\n","        prompt = hear_text()\n","\n","    if (prompt == \"Thank you.\"): # Whisper likes to return this when it's quiet\n","        continue\n","    if (prompt == \"Terminate.\" or prompt == \"terminate\"):\n","        break\n","    print(prompt)\n","    RenderImage(bonusPrompt+prompt)\n","    ShowImage(prompt)\n","print('Bye for now!')"]}]}